{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57d6e3fa-f558-4b09-8790-2c2770b841d3",
   "metadata": {},
   "source": [
    "## Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8945b2a-310c-4dc1-aaa6-edd8b2558d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas --quiet\n",
    "%pip install tabulate --quiet\n",
    "%pip install pdfplumber --quiet\n",
    "%pip install tqdm\n",
    "\n",
    "%pip install pymupdf --quiet --upgrade --prefer-binary --only-binary :all:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ae9d84-1a59-4dfd-a5d8-5d918dc07a9b",
   "metadata": {},
   "source": [
    "## Define the SkipFile exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d1a53-6892-4eee-a52b-b6c322419cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFile(Exception):\n",
    "    \"\"\"Raised to indicate this PDF should be skipped (no data to process).\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009b2fd9-74b7-40c3-865c-1ab161a2da19",
   "metadata": {},
   "source": [
    "## Define page-finder & table-extractor functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b40a4f3-5ad6-4234-8438-5c0e1616ca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fitz\n",
    "import pdfplumber\n",
    "\n",
    "def find_pages(pdf_path: str, keyword: str) -> list[int]:\n",
    "    \"\"\"\n",
    "    Return 1-based page numbers where `keyword` appears in the page text.\n",
    "    \"\"\"\n",
    "    pages = []\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for i in range(doc.page_count):\n",
    "            if keyword in doc[i].get_text():\n",
    "                pages.append(i + 1)\n",
    "    return pages\n",
    "\n",
    "\n",
    "def extract_tables(\n",
    "    pdf_path: str,\n",
    "    pages: list[int],\n",
    "    period: str,\n",
    "    drop_type: str = \"Spread\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    From the given pages, pull out every table row where\n",
    "    df['Period']==period AND df['Type']!=drop_type.\n",
    "    Returns one concatenated DataFrame, or raises SkipFile if none.\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for pnum in pages:\n",
    "            tbl = pdf.pages[pnum - 1].extract_table()\n",
    "            if not tbl:\n",
    "                continue\n",
    "            df = pd.DataFrame(tbl[1:], columns=tbl[0])\n",
    "            if \"Type\" in df.columns:\n",
    "                df[\"Type\"] = df[\"Type\"].str.strip()\n",
    "                df = df[df[\"Type\"] != drop_type]\n",
    "            if \"Period\" in df.columns:\n",
    "                df = df[df[\"Period\"] == period]\n",
    "            if not df.empty:\n",
    "                dfs.append(df)\n",
    "\n",
    "    if not dfs:\n",
    "        # nothing matched → skip this PDF\n",
    "        raise SkipFile(f\"No tables extracted\")\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8f1cfc-fa7b-4589-8f77-11159aa8b624",
   "metadata": {},
   "source": [
    "## Define a function to determine if \"In the Money\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df9b86-003f-4683-89fb-7685b9322e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_in_the_money(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert 'Exp Value' to numeric;\n",
    "    Extract the numeric strike price from 'Display Name' (number following '>');\n",
    "    Compute 'Flag' as 1 if Exp Value > Strike Price, else 0;\n",
    "    Keep the 'exp time' column unchanged.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .assign(\n",
    "            **{\n",
    "                'Exp Value': lambda df: pd.to_numeric(df['Exp Value'], errors='coerce'),\n",
    "                'Strike Price': lambda df: (\n",
    "                    df['Display Name']\n",
    "                      .str.extract(r'>\\s*([\\d.]+)', expand=False)\n",
    "                      .pipe(pd.to_numeric, errors='coerce')\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        .assign(\n",
    "            Flag=lambda df: (df['Exp Value'] > df['Strike Price']).astype(int)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ffa62a-2d12-4d41-a229-15ac58f06761",
   "metadata": {},
   "source": [
    "## Define the clean and rename function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1561923-5ae6-4fd3-bb21-1898d0fcdedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_and_rename(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    to_drop = [\n",
    "        c for c in [\n",
    "            \"Period\", \"Display Name\",\n",
    "            \"Type\", \"Buyer\", \"Seller\"\n",
    "        ] if c in df.columns\n",
    "    ]\n",
    "    return (\n",
    "        df\n",
    "        .rename(columns={\n",
    "            \"Business Date\": \"Date\",\n",
    "            \"Flag\": \"In the Money\",\n",
    "        })\n",
    "        .drop_duplicates(\n",
    "            subset=[\"Date\", \"Exp Value\", \"Strike Price\", \"Ticker\"],\n",
    "            keep=\"first\",\n",
    "        )\n",
    "        .drop(columns=to_drop)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f3b9cb-01f1-4578-aa3a-c8316d760965",
   "metadata": {},
   "source": [
    "## Define a function to substitue the Ticker for the name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92925eb1-7bec-46a7-a2b1-00a26f5c6811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "def add_ticker_from_mapping(\n",
    "    df: pd.DataFrame,\n",
    "    mapping_file: Union[str, Path]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a ticker mapping from YAML config file,\n",
    "    then add a 'Ticker' column to `df` by substring-matching 'Name' against each display name.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Source DataFrame containing a 'Name' column.\n",
    "    mapping_file : str or Path\n",
    "        Path to YAML file with ticker mappings (display_name: ticker_symbol)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A new DataFrame with an added 'Ticker' column.\n",
    "    \"\"\"\n",
    "    # Load ticker mappings from YAML\n",
    "    with open(mapping_file, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Build reverse mapping: display_name -> ticker_symbol\n",
    "    tickers = config.get('tickers', {})\n",
    "    display_to_ticker = {\n",
    "        info['display_name']: ticker_symbol\n",
    "        for ticker_symbol, info in tickers.items()\n",
    "    }\n",
    "    \n",
    "    def lookup_ticker(name: str) -> str:\n",
    "        if pd.isna(name):\n",
    "            return 'UNKNOWN'\n",
    "        # Check each display name in the mappings\n",
    "        for display_name, ticker in display_to_ticker.items():\n",
    "            if display_name in name:\n",
    "                return ticker\n",
    "        return 'UNKNOWN'\n",
    "\n",
    "    result = df.copy()\n",
    "    result['Ticker'] = result['Name'].apply(lookup_ticker)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340d398a-b853-42b4-b234-ea4c1205f83c",
   "metadata": {},
   "source": [
    "## Define a function to save the final result to an S3 CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b4dfb4-799e-456b-b202-aba3369fd976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import io\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def upload_df_to_s3(\n",
    "    df: pd.DataFrame,\n",
    "    s3_client,\n",
    "    bucket: str,\n",
    "    key: str,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Upload a DataFrame to S3 as CSV using your provided S3 client.\n",
    "    \"\"\"\n",
    "    bucket = bucket.strip()\n",
    "\n",
    "    # sanity‐check bucket exists\n",
    "    try:\n",
    "        s3_client.head_bucket(Bucket=bucket)\n",
    "    except ClientError as e:\n",
    "        raise RuntimeError(f\"Could not access bucket '{bucket}': {e}\") from e\n",
    "\n",
    "    # ——— Use BytesIO instead of StringIO ————————————————\n",
    "    csv_buffer = io.BytesIO()\n",
    "    # write UTF-8–encoded bytes, not text\n",
    "    csv_buffer.write(df.to_csv(index=False).encode('utf-8'))\n",
    "    csv_buffer.seek(0)\n",
    "\n",
    "    try:\n",
    "        s3_client.upload_fileobj(\n",
    "            Fileobj=csv_buffer,\n",
    "            Bucket=bucket,\n",
    "            Key=key,\n",
    "            ExtraArgs={\"ContentType\": \"text/csv\"},\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed to upload CSV to s3://{bucket}/{key}: {e}\"\n",
    "        ) from e\n",
    "\n",
    "    # optional confirm\n",
    "    try:\n",
    "        meta = s3_client.head_object(Bucket=bucket, Key=key)\n",
    "        # print(f\"   → Confirmed upload: {meta.get('ContentLength')} bytes\")\n",
    "    except Exception:\n",
    "        print(\"   ⚠️ Could not confirm upload with head_object\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e15ccff-7041-4b3f-a97c-77f8e0f4fdee",
   "metadata": {},
   "source": [
    "## Define function to avoid re-processing processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b83cd1-677b-4b98-9188-32eb05e0c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3 \n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "def load_manifest(\n",
    "    s3_client: boto3.client, \n",
    "    bucket_name: str, \n",
    "    manifest_key: str = \"manifests/processed_files.json\"\n",
    ") -> set[str]:\n",
    "    \"\"\"\n",
    "    Download and parse the JSON manifest of processed PDFs.\n",
    "    Returns a set of keys, or empty set if it doesn’t exist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = s3_client.get_object(Bucket=bucket_name, Key=manifest_key)\n",
    "        return set(json.loads(resp[\"Body\"].read()))\n",
    "    except ClientError as e:\n",
    "        # If the object isn’t found, return empty set; else re-raise\n",
    "        if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n",
    "            return set()\n",
    "        raise\n",
    "\n",
    "def save_manifest(\n",
    "    processed: set[str],\n",
    "    s3_client: boto3.client,\n",
    "    bucket_name: str,\n",
    "    manifest_key: str = \"manifests/processed_files.json\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Upload the updated manifest back to S3.\n",
    "    \"\"\"\n",
    "    s3_client.put_object(\n",
    "        Bucket=bucket_name,\n",
    "        Key=manifest_key,\n",
    "        Body=json.dumps(list(processed)).encode(\"utf-8\"),\n",
    "        ContentType=\"application/json\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63caae7-9e41-490e-93a7-01aab5404c49",
   "metadata": {},
   "source": [
    "## Define a function to get all the files related to trading results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d8af33-1d99-4cf8-b8b6-13ed7c691f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def list_nadex_trading_results(bucket, prefix: str = \"\") -> List[str]:\n",
    "    \"\"\"\n",
    "    List PDF keys in the given bucket under `prefix`\n",
    "    that contain 'tradingResults' and end with .pdf.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        obj.key\n",
    "        for obj in bucket.objects.filter(Prefix=prefix)\n",
    "        if \"tradingResults\" in obj.key and obj.key.lower().endswith(\".pdf\")\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1730e25-03ff-4f12-bc3d-9ba00bc4ce53",
   "metadata": {},
   "source": [
    "## Define functions to setup access to S3, and filter the list of pdf's returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612ddbef-2962-4a5b-9be0-80d8dffb75d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "\n",
    "from datetime import date, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import Iterable, List, Dict\n",
    "\n",
    "def create_s3_clients(\n",
    "    profile: str = \"default\", region: str = \"us-east-1\"\n",
    ") -> Dict[str, boto3.client]:\n",
    "    session = boto3.Session(profile_name=profile, region_name=region)\n",
    "    return {\n",
    "        \"public\": session.client(\n",
    "            \"s3\",\n",
    "            config=Config(signature_version=UNSIGNED),\n",
    "            region_name=region,\n",
    "        ),\n",
    "        \"private\": session.client(\"s3\"),\n",
    "        \"resource\": session.resource(\"s3\"),\n",
    "    }\n",
    "\n",
    "def get_bucket(resource: boto3.resource, name: str):\n",
    "    return resource.Bucket(name)\n",
    "\n",
    "def parse_key_date(key: str) -> date:\n",
    "    stem = Path(key).stem\n",
    "    return datetime.strptime(stem[:8], \"%Y%m%d\").date()\n",
    "\n",
    "def filter_new_pdfs(\n",
    "    keys: Iterable[str],\n",
    "    processed: Iterable[str],\n",
    "    start: date = date(2024, 1, 1),\n",
    "    end: date | None = None,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Return keys between start & end (inclusive) that aren’t in processed.\n",
    "    \"\"\"\n",
    "    if end is None:\n",
    "        end = date.today()\n",
    "    return [\n",
    "        key\n",
    "        for key in keys\n",
    "        if start <= (d := parse_key_date(key)) <= end\n",
    "        and key not in processed\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1510c0-0368-461a-820c-9a1c1fdb4377",
   "metadata": {},
   "source": [
    "## Define a helper function to process each file\n",
    "\n",
    "### Also, define a function to run the Pipeline. For each PDF in the folder:\n",
    "2. Read the PDF file\n",
    "3. Extract the Tables with Daily contracts\n",
    "4. Create a CSV\n",
    "5. Write the CSV to S3\n",
    "6. Log the PDF file as 'processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adad833-5c80-4d1b-b192-d07a3280d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import date\n",
    "from typing import List\n",
    "\n",
    "import traceback\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def _process_pdf(\n",
    "    pdf_key: str,\n",
    "    target: str,\n",
    "    mapping_file: Path,\n",
    "    public_s3,\n",
    "    private_s3,\n",
    "    bucket_name: str,\n",
    "    nadex_bucket_name: str,\n",
    "    tmp_dir: Path = Path(\"/tmp\"),\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Download a single PDF, extract/transform, upload the CSV, and\n",
    "    print a success message. Raises on any step failure.\n",
    "    \"\"\"\n",
    "    local_pdf = tmp_dir / Path(pdf_key).name\n",
    "    public_s3.download_file(\n",
    "        Bucket=nadex_bucket_name,\n",
    "        Key=pdf_key,\n",
    "        Filename=str(local_pdf),\n",
    "    )\n",
    "\n",
    "    def _format_ctx(exc: Exception) -> str:\n",
    "        tb = traceback.extract_tb(exc.__traceback__)\n",
    "        fn, ln, _, text = tb[-1]\n",
    "        return f\"{fn}:{ln} -> {text.strip()}\"\n",
    "        \n",
    "    try:\n",
    "        pages = find_pages(str(local_pdf), target)\n",
    "        df = (\n",
    "            extract_tables(str(local_pdf), pages, target)\n",
    "            .pipe(find_in_the_money)\n",
    "            .pipe(add_ticker_from_mapping, mapping_file)\n",
    "            .pipe(clean_and_rename)\n",
    "        )\n",
    "    except SkipFile as sf:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        ctx = _format_ctx(e)\n",
    "        raise RuntimeError(\n",
    "            f\"[EXTRACT/TRANSFORM ERROR] '{pdf_key}' (pages={pages}) \"\n",
    "            f\"({ctx}): {e}\"\n",
    "        ) from e\n",
    "\n",
    "    try:\n",
    "        upload_df_to_s3(df, private_s3, bucket_name, f\"historical/{Path(pdf_key).stem.split('_', 1)[0]}_Historical.csv\")\n",
    "    except Exception as e:\n",
    "        ctx = _format_ctx(e)\n",
    "        raise RuntimeError(\n",
    "            f\"[UPLOAD ERROR] '{pdf_key}' → '{bucket_name}/{pdf_key}' \"\n",
    "            f\"({ctx}): {e}\"\n",
    "        ) from e\n",
    "\n",
    "def run_nadex_pipeline(\n",
    "    mapping_file: Path,\n",
    "    target: str,\n",
    "    bucket_name: str,\n",
    "    nadex_bucket_name: str,\n",
    "    start: date,\n",
    "    end: date,\n",
    "):\n",
    "    \"\"\"\n",
    "    Orchestrates the full Nadex PDF → CSV pipeline over a given date range.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    mapping_file : Path to the CSV mapping file used to enrich extracted tables.\n",
    "    target : The keyword to locate pages within each PDF (e.g. \"Daily\").\n",
    "    bucket_name : Name of your own S3 bucket where resulting CSVs and the manifest are stored.\n",
    "    nadex_bucket_name : Name of the public Nadex S3 bucket from which PDFs are downloaded (unsigned).\n",
    "    start : Lower bound (inclusive) on PDF dates to process (parsed from filenames).\n",
    "    end : Upper bound (inclusive) on PDF dates to process.\n",
    "\n",
    "    Actions:\n",
    "    --------\n",
    "    1. Bootstraps three S3 interfaces:\n",
    "       - `public_s3`: an unsigned client to download Nadex PDFs.\n",
    "       - `private_s3`: a signed client for uploading CSVs & manifest.\n",
    "       - `s3_resource`: resource interface to enumerate bucket objects.\n",
    "    2. Constructs `buckets` dict with Bucket objects for both source (market) and destination.\n",
    "    3. Loads the JSON “processed files” manifest from your private bucket into a `processed` set.\n",
    "    4. Iterates over every PDF key in the Nadex (market) bucket:\n",
    "       a. Skips any key already in `processed`, accumulating in `skipped`.\n",
    "       b. Parses the date out of the filename and skips if outside `[start, end]`.\n",
    "       c. Calls `_process_pdf(...)` to:\n",
    "          • Download PDF locally,\n",
    "          • Extract & transform tables,\n",
    "          • Enrich with ticker mapping,\n",
    "          • Upload the resulting CSV back to your bucket.\n",
    "       d. On success, adds the key to `processed`; on exception, logs to `errors`.\n",
    "    5. After the loop finishes, writes the updated `processed` manifest back to S3.\n",
    "    6. Prints a summary of how many files were processed, skipped, and errored.\n",
    "\n",
    "    \"\"\"\n",
    "    number_processed = number_skipped = number_of_errors = 0\n",
    "    \n",
    "    clients = create_s3_clients()\n",
    "    public_s3 = clients[\"public\"]\n",
    "    private_s3 = clients[\"private\"]\n",
    "    s3_resource = clients[\"resource\"]\n",
    "    buckets = {\n",
    "        \"daily\":  get_bucket(s3_resource, bucket_name),\n",
    "        \"market\": get_bucket(s3_resource, nadex_bucket_name),\n",
    "    }\n",
    "\n",
    "    processed = load_manifest(private_s3, bucket_name)\n",
    "    errors: Dict[str, str]  = {}\n",
    "\n",
    "    # 1) List *all* PDFs in the Nadex bucket\n",
    "    all_keys = list_nadex_trading_results(buckets[\"market\"], prefix=\"\")\n",
    "\n",
    "    # 2) Filter those to your date window\n",
    "    date_range_keys = [\n",
    "        k for k in all_keys\n",
    "        if start <= parse_key_date(k) <= end\n",
    "    ]\n",
    "\n",
    "    # 3) Split into new vs skipped-within-date-range\n",
    "    new_keys = []\n",
    "    skipped: Dict[str, str] = {}\n",
    "    \n",
    "    for k in date_range_keys:\n",
    "        if k in processed:\n",
    "            skipped[k] = \"already processed\"\n",
    "            number_skipped += 1\n",
    "        else:\n",
    "            new_keys.append(k)\n",
    "            number_processed += 1\n",
    "\n",
    "    errors: Dict[str, str] = {}\n",
    "    \n",
    "    for pdf_key in tqdm(\n",
    "        new_keys,\n",
    "        desc=\"Processing PDFs\",\n",
    "        ascii=True,\n",
    "    ):\n",
    "\n",
    "    # ─── Main loop (single statement!) ────────────────────────────────────────\n",
    "        try:\n",
    "            _process_pdf(\n",
    "                pdf_key,\n",
    "                target,\n",
    "                mapping_file,\n",
    "                public_s3,\n",
    "                private_s3,\n",
    "                bucket_name,\n",
    "                nadex_bucket_name,\n",
    "            )\n",
    "            processed.add(pdf_key)\n",
    "        except SkipFile as sf:\n",
    "            skipped[pdf_key] = str(sf)\n",
    "            number_skipped += 1\n",
    "        except Exception as exc:\n",
    "            errors[pdf_key] = str(exc)\n",
    "            number_of_errors += 1\n",
    "\n",
    "    save_manifest(processed, private_s3, bucket_name)\n",
    "\n",
    "    print()\n",
    "    print(f\"Done — processed: {len(processed)} files\")\n",
    "    print(f\"       skipped:   {len(skipped)} files already processed\")\n",
    "    print(f\"       errors:    {len(errors)} failures\")\n",
    "\n",
    "    return {\n",
    "        'files_processed': number_processed,\n",
    "        'files_skipped': number_skipped,\n",
    "        'files_with_errors': number_of_errors \n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c659b17-58b5-4a37-88c2-dde5f033ec22",
   "metadata": {},
   "source": [
    "## Record in the Run Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798b7deb-190c-4394-84b8-620da0a61efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "with open('../configs/s3.yaml', 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "    \n",
    "# Run log helper (append a row to S3 CSV)\n",
    "import io, csv, datetime as dt\n",
    "from botocore.exceptions import ClientError  # comes with boto3\n",
    "\n",
    "RUNLOG_FIELDS = [\n",
    "    'date','start_time','end_time','status',\n",
    "    'files_processed','files_skipped','files_error',\n",
    "    'run_id','notes'\n",
    "]\n",
    "\n",
    "from botocore.config import Config\n",
    "\n",
    "session = boto3.Session(region_name=cfg.get('region'))\n",
    "private_s3 = session.client('s3')\n",
    "public_s3 = session.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "def append_runlog_s3(\n",
    "    bucket: str,\n",
    "    key: str,\n",
    "    *,\n",
    "    start_time=None,\n",
    "    status='success',\n",
    "    files_processed=0,\n",
    "    files_skipped=0,\n",
    "    files_error=0,\n",
    "    run_id='',\n",
    "    notes=''\n",
    "):\n",
    "    now = dt.datetime.now()\n",
    "    start = start_time or now\n",
    "\n",
    "    row = {\n",
    "        'date': now.date().isoformat(),\n",
    "        'start_time': start if isinstance(start, str) else start.isoformat(timespec='seconds'),\n",
    "        'end_time': now.isoformat(timespec='seconds'),\n",
    "        'status': status,\n",
    "        'files_processed': int(files_processed),\n",
    "        'files_skipped': int(files_skipped),\n",
    "        'files_error': int(files_error),\n",
    "        'run_id': run_id,\n",
    "        'notes': notes,\n",
    "    }\n",
    "\n",
    "    # Fetch existing (if present)\n",
    "    buf = io.StringIO()\n",
    "    need_header = False\n",
    "    try:\n",
    "        obj = private_s3.get_object(Bucket=bucket, Key=key)\n",
    "        buf.write(obj['Body'].read().decode('utf-8'))\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] in ('NoSuchKey', '404'):\n",
    "            need_header = True\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    if buf.tell() == 0:\n",
    "        need_header = True\n",
    "\n",
    "    writer = csv.DictWriter(buf, fieldnames=RUNLOG_FIELDS)\n",
    "    if need_header:\n",
    "        writer.writeheader()\n",
    "    if buf.getvalue() and not buf.getvalue().endswith(''):\n",
    "        buf.write('')\n",
    "    writer.writerow(row)\n",
    "\n",
    "    private_s3.put_object(\n",
    "        Bucket=bucket,\n",
    "        Key=key,\n",
    "        Body=buf.getvalue().encode('utf-8'),\n",
    "        ContentType='text/csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bedcc3-71c1-483a-9f2c-fe6212a64e3c",
   "metadata": {},
   "source": [
    "## Run the pipeline and show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328250b5-caa5-433e-81a1-649d37357f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "with open('../configs/s3.yaml', 'r') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "BUCKET = cfg['bucket']\n",
    "PUBLIC_BUCKET = cfg['public_bucket']\n",
    "HIST_PREFIX = cfg['prefixes']['historical']\n",
    "MANIFEST_KEY = f\"{cfg['prefixes']['manifests']}/processed_files.json\"\n",
    "RUNLOG_KEY = f\"{cfg['prefixes']['logs']}/run_log.csv\"\n",
    "\n",
    "# Use ticker_mappings.yaml instead of CSV\n",
    "MAPPING_FILE = Path('../configs/ticker_mappings.yaml')\n",
    "\n",
    "# Append a run-log row to S3 CSV\n",
    "import io, csv, datetime as dt\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "\n",
    "metrics = run_nadex_pipeline(\n",
    "    mapping_file=MAPPING_FILE,\n",
    "    target=\"Daily\",\n",
    "    bucket_name=BUCKET,\n",
    "    nadex_bucket_name=PUBLIC_BUCKET,\n",
    "    start=date(2025, 3, 1),\n",
    "    end=date.today(),\n",
    ")\n",
    "\n",
    "run_start = dt.datetime.now()\n",
    "\n",
    "# Generate a run_id (timestamp) for provenance (no Git required)\n",
    "run_id = run_start.strftime(\"%Y%m%dT%H%M%S\")\n",
    "\n",
    "# After run: append run log with counters\n",
    "append_runlog_s3(\n",
    "    BUCKET, RUNLOG_KEY,\n",
    "    start_time=run_start,\n",
    "    status=('success' if metrics.get('files_error', 0) == 0 else\n",
    "            'partial' if metrics.get('files_processed', 0) > 0 else 'failed'),\n",
    "    files_processed=metrics.get('files_processed', 0),\n",
    "    files_skipped=metrics.get('files_skipped', 0),\n",
    "    files_error=metrics.get('files_error', 0),\n",
    "    run_id=run_id,\n",
    "    notes='Historical batch run'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda (py312)",
   "language": "python",
   "name": "py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
